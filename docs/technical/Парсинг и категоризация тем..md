# **Аналитический Отчет: Функционал Парсинга Adobe Stock и Категоризации Тем**

**Версия документа: 1.0** **Дата: 16.10.2025** **Статус: Post-MVP (Планируется к реализации после запуска базовой версии)**

## **1\. Обзор и Бизнес-цель**

Данный функционал является ключевым улучшением для платных тарифов (PRO и ULTRA) и представляет собой переход от простого статистического анализа CSV-отчетов к **глубокой качественной аналитике**.

**Основная цель** — автоматически извлекать ключевые слова (теги) с сайта Adobe Stock для самых продаваемых работ пользователя, анализировать их с помощью большой языковой модели (LLM) и предоставлять пользователю отчет не просто о "топовых файлах", а о **"топовых темах и концепциях"**, которые приносят ему наибольший доход.

Это позволяет авторам получать **конкретные, действенные инсайты** для планирования будущих съемок и генераций, масштабируя успешные направления.

## **2\. Технологический Стек**

Для реализации этого сложного пайплайна выбраны следующие технологии:

### **2.1. Веб-скрапинг (Сбор данных)**

* **Инструмент:** **Playwright** (в асинхронном режиме).  
* **Обоснование:** Сайт Adobe Stock является современным веб\-приложением, которое активно использует JavaScript для рендеринга контента, включая блок с тегами. Простые парсеры (как BeautifulSoup) не могут исполнять JS и не увидят нужную информацию. Playwright управляет полноценным браузером, что гарантирует доступ к финальному HTML-коду страницы после выполнения всех скриптов.

### **2.2. Категоризация и Анализ (Обработка данных)**

* **Инструмент:** **API большой языковой модели (LLM)**, предпочтительно **Anthropic Claude 3.5 Sonnet**.  
* **Обоснование:** Модели Claude 3.5 Sonnet выбраны за оптимальное соотношение цены, скорости и качества. Они отлично справляются с задачами анализа текста, кластеризации и, что критически важно, могут возвращать результат в строго структурированном формате **JSON** по заданной схеме. Это позволяет надежно интегрировать AI-аналитику в автоматизированный процесс.

## **3\. Пошаговый Пайплайн Обработки**

Процесс разделен на несколько асинхронных шагов, выполняемых в фоновом режиме с помощью воркеров **Dramatiq**, чтобы не блокировать основной поток работы бота.

1. **Инициация:** После успешной обработки CSV-файла для пользователя с платным тарифом, система идентифицирует топ-5 (PRO) или топ-10 (ULTRA) работ по доходу.  
2. **Формирование URL:** Для каждой топовой работы генерируется прямая ссылка на страницу Adobe Stock по формату: https://stock.adobe.com/images/{slug}/{ID}.  
   * {ID} — это ID работы из CSV.  
   * {slug} — это название работы (Title), преобразованное в URL-совместимый формат (нижний регистр, замена пробелов и спецсимволов на дефисы).  
3. **Скрапинг Тегов:** Запускается фоновый актор scrape\_asset\_tags, который:  
   * Использует **Playwright** для перехода по сформированному URL.  
   * Ожидает полной загрузки страницы и появления блока с ключевыми словами.  
   * Извлекает все теги (ключевые слова) и сохраняет их, привязывая к ID ассета.  
   * **Кэширование:** Результаты скрапинга кэшируются в базе данных (таблица asset\_details), чтобы избежать повторных запросов для одной и той же работы в будущем.  
4. **Категоризация с помощью LLM:** Запускается второй фоновый актор llm\_cluster\_themes, который:  
   * Собирает все теги, полученные на предыдущем шаге для данного анализа.  
   * Формирует единый промпт для **Claude 3.5 Sonnet**, передавая весь список тегов.  
   * **Промпт содержит четкую инструкцию:** "Проанализируй этот список ключевых слов от разных изображений. Сгруппируй их в 1-2 основные коммерческие темы для каждой работы. Объедини похожие темы. Верни результат в формате JSON со следующей структурой: {'theme': 'название темы', 'sales': X, 'revenue': Y}".  
5. **Агрегация и Сохранение:**  
   * Система получает и валидирует JSON-ответ от LLM.  
   * Данные агрегируются: доходы и продажи по одинаковым темам суммируются.  
   * Результат (список самых прибыльных тем) сохраняется в базу данных (analysis\_top\_assets или отдельную таблицу для тем).  
6. **Формирование Глобальной Базы Тем:**  
   * Все успешно категоризированные темы анонимно добавляются в общую базу (anonymized\_theme\_performance).  
   * Эта база агрегирует данные от всех пользователей, позволяя в будущем выявлять глобальные тренды и использовать их для раздела "Темы и тренды".

## **4\. Риски и Меры по их Смягчению**

* **Риск:** Adobe Stock может заблокировать IP-адрес сервера за скрапинг.  
  * **Смягчение:** Использование ротируемых прокси-серверов и смена User-Agent в Playwright для маскировки запросов под действия обычного пользователя.  
* **Риск:** Изменение верстки сайта Adobe Stock, что сломает логику парсера.  
  * **Смягчение:** Использование надежных селекторов (например, по data-testid, если они есть), настройка мониторинга и алертов на увеличение ошибок парсинга для быстрой реакции и обновления кода.  
* **Риск:** Высокая стоимость и нестабильность ответов от LLM.  
  * **Смягчение:** Агрессивное кэширование результатов, логирование каждого API-вызова для контроля расходов. Промпт должен быть тщательно спроектирован для получения стабильного JSON-формата.  
* **Риск:** Длительное время выполнения всего пайплайна.  
  * **Смягчение:** Весь процесс выполняется асинхронно в фоне. Пользователь получает уведомление о готовности, а не ждет в реальном времени. Горизонтальное масштабирование воркеров Dramatiq позволит обрабатывать больше задач одновременно.

